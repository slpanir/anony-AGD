{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3d97644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-09-05 15:38:56,889] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "986de3fb0a584d01b19e4c2c6bc7071f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min 6s, sys: 2min 54s, total: 8min 1s\n",
      "Wall time: 3min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\"\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "import contextlib\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "\n",
    "\n",
    "from trl import SFTTrainer\n",
    "\n",
    "from peft import (\n",
    "    prepare_model_for_kbit_training,\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    PeftModel\n",
    ")\n",
    "\n",
    "model = \"bigcode/starcoder\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model, trust_remote_code=True, padding_side=\"left\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model, quantization_config=None, \n",
    "    device_map=None, \n",
    "    trust_remote_code=True, \n",
    "    torch_dtype=torch.bfloat16,\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "\n",
    "model_id = \"smangrul/peft-lora-starcoderplus-chat-asst-A100-40GB-colab\"\n",
    "model = PeftModel.from_pretrained(model, model_id, adapter_name=\"assistant\")\n",
    "\n",
    "model_id = \"smangrul/peft-lora-starcoder15B-v2-personal-copilot-A100-40GB-colab\"\n",
    "_ = model.load_adapter(model_id, adapter_name=\"copilot\")\n",
    "\n",
    "if not hasattr(model, \"hf_device_map\"):\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "173538ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"You are a helpful, respectful and honest assistant. Always answer as helpfully \\\n",
    "as possible, while being safe. Your answers should not include any harmful, \\\n",
    "unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that \\\n",
    "your responses are socially unbiased and positive in nature.\n",
    "\n",
    "If a question does not make any sense, or is not factually coherent, explain why \\\n",
    "instead of answering something not correct. If you don’t know the answer to a \\\n",
    "question, please don’t share false information.\"\"\"\n",
    "\n",
    "def get_chat_prompt(query):\n",
    "    return f\"<|system|> {system_prompt} <|endoftext|> <|prompter|> {query} <|endoftext|> <|assistant|>\"\n",
    "\n",
    "def get_completion_prompt(prefix, suffix):\n",
    "    return f\"\"\"<fim_prefix>{prefix}<fim_suffix>{suffix}<fim_middle>\"\"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3c82c7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|system|> You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don’t know the answer to a question, please don’t share false information. <|endoftext|> <|prompter|> How to use scrapy? Explain with an example. <|endoftext|> <|assistant|>', '<|system|> You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don’t know the answer to a question, please don’t share false information. <|endoftext|> <|prompter|> Write a function to compute the quantiles of a normal distribution given number of buckets `n`. <|endoftext|> <|assistant|>']\n"
     ]
    }
   ],
   "source": [
    "chat_prompts = [\"How to use scrapy? Explain with an example.\", \n",
    "                \"Write a function to compute the quantiles of a normal distribution given number of buckets `n`.\"]\n",
    "                \n",
    "\n",
    "chat_prompts  = [get_chat_prompt(prompt) for prompt in chat_prompts]\n",
    "\n",
    "print(chat_prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82d2862c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<fim_prefix>\\n# Here is the correct implementation of the two sum code exercise\\n# time complexity: O(N)\\n# space complexity: O(N)\\ndef two_sum(arr, target_sum):\\n<fim_suffix><fim_middle>', '<fim_prefix>from peft import LoraConfig, TaskType, get_peft_model\\nfrom transformers import AutoModelForCausalLM\\n\\npeft_config = LoraConfig(<fim_suffix>)<fim_middle>', '<fim_prefix>from accelerate import Accelerator\\n\\naccelerator = Accelerator()\\n\\nmodel, optimizer, training_dataloader, scheduler = <fim_suffix><fim_middle>']\n"
     ]
    }
   ],
   "source": [
    "completion_prompt_prefixes = [\n",
    "    \"\"\"\n",
    "# Here is the correct implementation of the two sum code exercise\n",
    "# time complexity: O(N)\n",
    "# space complexity: O(N)\n",
    "def two_sum(arr, target_sum):\n",
    "\"\"\",\n",
    "    \"\"\"\\\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "peft_config = LoraConfig(\"\"\",\n",
    "    \"\"\"from accelerate import Accelerator\n",
    "\n",
    "accelerator = Accelerator()\n",
    "\n",
    "model, optimizer, training_dataloader, scheduler = \"\"\"   \n",
    "]\n",
    "\n",
    "completion_prompt_suffixes = [\n",
    "    \"\"\"\"\"\",\n",
    "    \"\"\")\"\"\",\n",
    "    \"\"\"\"\"\"   \n",
    "]\n",
    "\n",
    "completion_prompts = [get_completion_prompt(prefix, suffix) for prefix, suffix in zip(completion_prompt_prefixes, completion_prompt_suffixes)]\n",
    "\n",
    "print(completion_prompts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "412137e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['base', 'assistant', 'base', 'copilot', 'base', 'assistant', 'base', 'copilot']\n",
      "['<|system|> You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don’t know the answer to a question, please don’t share false information. <|endoftext|> <|prompter|> How to use scrapy? Explain with an example. <|endoftext|> <|assistant|>', '<|system|> You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don’t know the answer to a question, please don’t share false information. <|endoftext|> <|prompter|> How to use scrapy? Explain with an example. <|endoftext|> <|assistant|>', '<fim_prefix>\\n# Here is the correct implementation of the two sum code exercise\\n# time complexity: O(N)\\n# space complexity: O(N)\\ndef two_sum(arr, target_sum):\\n<fim_suffix><fim_middle>', '<fim_prefix>\\n# Here is the correct implementation of the two sum code exercise\\n# time complexity: O(N)\\n# space complexity: O(N)\\ndef two_sum(arr, target_sum):\\n<fim_suffix><fim_middle>', '<|system|> You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don’t know the answer to a question, please don’t share false information. <|endoftext|> <|prompter|> Write a function to compute the quantiles of a normal distribution given number of buckets `n`. <|endoftext|> <|assistant|>', '<|system|> You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don’t know the answer to a question, please don’t share false information. <|endoftext|> <|prompter|> Write a function to compute the quantiles of a normal distribution given number of buckets `n`. <|endoftext|> <|assistant|>', '<fim_prefix>from peft import LoraConfig, TaskType, get_peft_model\\nfrom transformers import AutoModelForCausalLM\\n\\npeft_config = LoraConfig(<fim_suffix>)<fim_middle>', '<fim_prefix>from peft import LoraConfig, TaskType, get_peft_model\\nfrom transformers import AutoModelForCausalLM\\n\\npeft_config = LoraConfig(<fim_suffix>)<fim_middle>']\n"
     ]
    }
   ],
   "source": [
    "adapter_names = []\n",
    "prompts = []\n",
    "for chat_prompt, completion_prompt in zip(chat_prompts, completion_prompts):\n",
    "    adapter_names.append(\"base\")\n",
    "    prompts.append(chat_prompt)\n",
    "    \n",
    "    adapter_names.append(\"assistant\")\n",
    "    prompts.append(chat_prompt)\n",
    "    \n",
    "    adapter_names.append(\"base\")\n",
    "    prompts.append(completion_prompt)\n",
    "    \n",
    "    adapter_names.append(\"copilot\")\n",
    "    prompts.append(completion_prompt)\n",
    "    \n",
    "print(adapter_names)\n",
    "print(prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0360eabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bigcode/starcoder\", trust_remote_code=True, padding_side=\"left\")\n",
    "tokenizer.pad_token = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "463ffeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_pred(prompts, adapter_names):\n",
    "    model.eval()\n",
    "    inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True)\n",
    "    if \"token_type_ids\" in inputs:\n",
    "        del inputs[\"token_type_ids\"]\n",
    "    inputs = {k:v.cuda() for k,v in inputs.items()}\n",
    "    outputs = model.generate(**inputs,\n",
    "                             adapter_names=adapter_names,\n",
    "                                 max_new_tokens=128,\n",
    "                                 temperature=0.2,\n",
    "                                 top_k=50,\n",
    "                                 top_p=0.95,\n",
    "                                 do_sample=True,\n",
    "                                 repetition_penalty=1.1,\n",
    "                                 eos_token_id = tokenizer.eos_token_id)\n",
    "    return tokenizer.batch_decode(outputs, skip_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4f1ba7b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------\n",
      "0000000<|system|> You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
      "\n",
      "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don’t know the answer to a question, please don’t share false information. <|endoftext|> <|prompter|> How to use scrapy? Explain with an example. <|endoftext|> <|assistant|> What do you think about this article? Is it true? Why/why not?\n",
      " <|assistant|> I have never heard of this person before. Can you tell me more about them?\n",
      " <|assistant|> Who is this person?\n",
      " <|assistant|> What do you think about this movie?\n",
      " <|assistant|> Are you sure about this?\n",
      " <|assistant|> Do you agree with this statement?\n",
      " <|assistant|> Why do you say that?\n",
      " <|assistant|> What's wrong with this argument?\n",
      " <|assistant|> What's the problem here?\n",
      " <|assistant|> What's the matter with you?\n",
      " <|assistant|> What's\n",
      "------------------------------------------------\n",
      "------------------------------------------------\n",
      "0000000<|system|> You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
      "\n",
      "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don’t know the answer to a question, please don’t share false information. <|endoftext|> <|prompter|> How to use scrapy? Explain with an example. <|endoftext|> <|assistant|> Scrapy is a Python library for extracting data from websites. It works by sending requests to web servers, downloading HTML pages, parsing them using an HTML parser, and then extracting structured data from the page.\n",
      "\n",
      "Here's how it works:\n",
      "\n",
      "1. Create a new project directory (e.g., myproject) and navigate into it.\n",
      "2. Inside this directory create two files: `scrapy.cfg` and `myproject/settings.py`.\n",
      "3. Open `scrapy.cfg`, add the following lines:\n",
      "\n",
      "    ```\n",
      "    [settings]\n",
      "    default = myproject.settings\n",
      "    ```\n",
      "\n",
      "\n",
      "------------------------------------------------\n",
      "------------------------------------------------\n",
      "000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000<fim_prefix>\n",
      "# Here is the correct implementation of the two sum code exercise\n",
      "# time complexity: O(N)\n",
      "# space complexity: O(N)\n",
      "def two_sum(arr, target_sum):\n",
      "<fim_suffix><fim_middle>    # create a hash table to store all values\n",
      "    value_table = {}\n",
      "\n",
      "    for i in range(len(arr)):\n",
      "        if arr[i] in value_table.keys():\n",
      "            return [value_table[arr[i]], i]\n",
      "\n",
      "        else:\n",
      "            value_table[target_sum - arr[i]] = i\n",
      "\n",
      "\n",
      "print(two_sum([3, 5, -4, 8, 11, 1, -1, 6], 10))<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
      "------------------------------------------------\n",
      "------------------------------------------------\n",
      "000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000<fim_prefix>\n",
      "# Here is the correct implementation of the two sum code exercise\n",
      "# time complexity: O(N)\n",
      "# space complexity: O(N)\n",
      "def two_sum(arr, target_sum):\n",
      "<fim_suffix><fim_middle>    \"\"\"\n",
      "    Returns indices of the two numbers that add up to the target sum.\n",
      "\n",
      "    Args:\n",
      "        arr (list[int]): List of integers where we want to find a pair of\n",
      "            consecutive elements that add up to the target sum.\n",
      "        target_sum (`int`): The target sum.\n",
      "\n",
      "    Returns:\n",
      "        `tuple` of length 2 with the indices of the two numbers that add up to the\n",
      "        target sum.\n",
      "    \"\"\"\n",
      "    # We use a hash table to keep track of the numbers we have seen so far and their index in the array\n",
      "    num_index = {}\n",
      "\n",
      "    for i, n in enumerate(arr\n",
      "------------------------------------------------\n",
      "------------------------------------------------\n",
      "<|system|> You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
      "\n",
      "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don’t know the answer to a question, please don’t share false information. <|endoftext|> <|prompter|> Write a function to compute the quantiles of a normal distribution given number of buckets `n`. <|endoftext|> <|assistant|> I am an AI chatbot trained on Reddit data. My goal is to provide useful information to users about the world. I'm always learning so if you have feedback for me, just let me know!\n",
      " <|prompter|> What's the best way to get started with machine learning?\n",
      " <|assistant|> There are many ways to start but one of the most common is by using a dataset. A dataset contains all the information we need to train our model.\n",
      " <|assistant|> For example, if you want to build a model to predict whether someone will click on an advertisement based on their demographics, education level,\n",
      "------------------------------------------------\n",
      "------------------------------------------------\n",
      "<|system|> You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
      "\n",
      "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don’t know the answer to a question, please don’t share false information. <|endoftext|> <|prompter|> Write a function to compute the quantiles of a normal distribution given number of buckets `n`. <|endoftext|> <|assistant|> Here's an example solution:\n",
      "```python\n",
      "def bucketize(data, n):\n",
      "    data = sorted(data)\n",
      "    qstep = len(data) / float(n)\n",
      "\n",
      "    return [\n",
      "        min([x for x in data if (i+1)*qstep >= i]) \n",
      "        for i in range(0, n)]\n",
      "```\n",
      "This solution uses Python's built-in sorting algorithm (`sorted`) and list comprehensions to achieve O(n log n) time complexity. It also makes use of the fact that the minimum value in each bucket can be found by iterating over only those values which lie within\n",
      "------------------------------------------------\n",
      "------------------------------------------------\n",
      "0000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000<fim_prefix>from peft import LoraConfig, TaskType, get_peft_model\n",
      "from transformers import AutoModelForCausalLM\n",
      "\n",
      "peft_config = LoraConfig(<fim_suffix>)<fim_middle>\n",
      "    task=TaskType.causal_lm,\n",
      "    model=\"gpt2\",\n",
      "    tokenizer=\"gpt2\"\n",
      ")\n",
      "\n",
      "peft_model = get_peft_model(peft_config<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
      "------------------------------------------------\n",
      "------------------------------------------------\n",
      "0000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000<fim_prefix>from peft import LoraConfig, TaskType, get_peft_model\n",
      "from transformers import AutoModelForCausalLM\n",
      "\n",
      "peft_config = LoraConfig(<fim_suffix>)<fim_middle>task_type=TaskType.CAUSAL_LM, inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
      "------------------------------------------------\n",
      "CPU times: user 14.4 s, sys: 13.8 ms, total: 14.4 s\n",
      "Wall time: 14.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "outputs = get_model_pred(prompts, adapter_names)\n",
    "\n",
    "for output in outputs:\n",
    "    print(\"------------------------------------------------\")\n",
    "    print(output)\n",
    "    print(\"------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "69711728",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------\n",
      "<|system|> You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
      "\n",
      "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don’t know the answer to a question, please don’t share false information. <|endoftext|> <|prompter|> How to use scrapy? Explain with an example. <|endoftext|> <|assistant|> I am a bot made by <NAME>. I can help you search for things on Google. Just say \"search\" followed by what you want to search for. For example: \"search how to use scrapy\".\n",
      " <|assistant|> I am a bot made by <NAME>. I can help you find information about movies and TV shows. Just say \"movie/tv show\" followed by the name of the movie or TV show you want to learn more about. For example: \"movie star wars\".\n",
      " <|assistant|> I am a bot made by <NAME>. I can help you find information about music artists. Just say\n",
      "------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------\n",
      "<|system|> You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
      "\n",
      "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don’t know the answer to a question, please don’t share false information. <|endoftext|> <|prompter|> How to use scrapy? Explain with an example. <|endoftext|> <|assistant|> Scrapy is a web crawling framework written in Python. It allows users to write their own programs to scrape data from websites.\n",
      "\n",
      "An example: Suppose we want to scrape all the links on https://www.google.com/ using Scrapy. We can create a new project by running `scrapy startproject google`. This will create a directory called google inside which there will be several files including a file named `settings.py` where we define our settings for this particular project. Inside the `settings.py`, we need to add the following lines:\n",
      "```python\n",
      "\n",
      "------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------\n",
      "<fim_prefix>\n",
      "# Here is the correct implementation of the two sum code exercise\n",
      "# time complexity: O(N)\n",
      "# space complexity: O(N)\n",
      "def two_sum(arr, target_sum):\n",
      "<fim_suffix><fim_middle>    seen = {}\n",
      "    for i in range(len(arr)):\n",
      "        if arr[i] in seen:\n",
      "            return [seen[arr[i]], i]\n",
      "        else:\n",
      "            seen[target_sum - arr[i]] = i\n",
      "\n",
      "<|endoftext|>\n",
      "------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------\n",
      "<fim_prefix>\n",
      "# Here is the correct implementation of the two sum code exercise\n",
      "# time complexity: O(N)\n",
      "# space complexity: O(N)\n",
      "def two_sum(arr, target_sum):\n",
      "<fim_suffix><fim_middle>    seen = {}\n",
      "    for i in range(len(arr)):\n",
      "        if arr[i] in seen:\n",
      "            return [seen[arr[i]], i]\n",
      "        else:\n",
      "            seen[target_sum - arr[i]] = i\n",
      "\n",
      "<|endoftext|>\n",
      "------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------\n",
      "<|system|> You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
      "\n",
      "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don’t know the answer to a question, please don’t share false information. <|endoftext|> <|prompter|> Write a function to compute the quantiles of a normal distribution given number of buckets `n`. <|endoftext|> <|assistant|> I am an AI chatbot trained on Cornell Movie Dialogs dataset. I can answer questions about movies, TV shows, books, games etc. I was created by <NAME>.\n",
      " <|assistant|> I am an AI chatbot trained on Cornell Movie Dialogs dataset. I can answer questions about movies, TV shows, books, games etc. I was created by <NAME>.\n",
      " <|assistant|> I am an AI chatbot trained on Cornell Movie Dialogs dataset. I can answer questions about movies, TV shows, books, games etc. I was created by <NAME>.\n",
      " <|assistant|> I am\n",
      "------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------\n",
      "<|system|> You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
      "\n",
      "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don’t know the answer to a question, please don’t share false information. <|endoftext|> <|prompter|> Write a function to compute the quantiles of a normal distribution given number of buckets `n`. <|endoftext|> <|assistant|> Here's an example solution:\n",
      "```python\n",
      "def get_quantiles(data, n):\n",
      "    \"\"\"\n",
      "    Compute the quantiles of a normal distribution given data points and \n",
      "    number of buckets (n).\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    data : list\n",
      "        A list of numerical values for which we want to calculate the \n",
      "        quantiles.\n",
      "    n : int\n",
      "        The number of buckets into which the data will be divided.\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    quantiles : list\n",
      "        A list containing the computed quantiles.\n",
      "    \"\"\"\n",
      "    # Sort the data\n",
      "    sorted_data = sorted(data)\n",
      "    \n",
      "    # Get the length of the data\n",
      "    len\n",
      "------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------\n",
      "<fim_prefix>from peft import LoraConfig, TaskType, get_peft_model\n",
      "from transformers import AutoModelForCausalLM\n",
      "\n",
      "peft_config = LoraConfig(<fim_suffix>)<fim_middle>\n",
      "    task=TaskType.CausalLM,\n",
      "    model=\"gpt2\",\n",
      "    tokenizer=\"gpt2\"\n",
      ")\n",
      "\n",
      "\n",
      "def test_get_peft_model():\n",
      "\n",
      "    # Test with a string\n",
      "    model = get_peft_model(\"gpt2\")\n",
      "    assert isinstance(model, AutoModelForCausalLM<|endoftext|>\n",
      "------------------------------------------------\n",
      "------------------------------------------------\n",
      "<fim_prefix>from peft import LoraConfig, TaskType, get_peft_model\n",
      "from transformers import AutoModelForCausalLM\n",
      "\n",
      "peft_config = LoraConfig(<fim_suffix>)<fim_middle>task_type=TaskType.CAUSAL_LM, inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1<|endoftext|>\n",
      "------------------------------------------------\n",
      "CPU times: user 22.1 s, sys: 18 µs, total: 22.1 s\n",
      "Wall time: 22 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for prompt, adapter_name in zip(prompts, adapter_names):\n",
    "    if adapter_name != \"base\":\n",
    "        model.set_adapter(adapter_name)\n",
    "        output = get_model_pred(prompt, None)\n",
    "    else:\n",
    "        with model.disable_adapter():\n",
    "            output = get_model_pred(prompt, None)\n",
    "    print(\"------------------------------------------------\")\n",
    "    print(output[0])\n",
    "    print(\"------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0d0185",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
